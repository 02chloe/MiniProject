{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This part is trying to build an A2C model for trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import preprocess_env3_8features\n",
    "import preprocessing_adding_technical_indicator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor nn\n",
    "class Actor(object):\n",
    "\n",
    "    def __init__(self, state_dim =8, action_dim =3, lr=0.001):\n",
    "        \n",
    "\n",
    "        input_layer = tl.layers.Input([None, state_dim])\n",
    "        layer = tl.layers.Dense(n_units=16, act=tf.nn.relu6)(input_layer)\n",
    "        layer = tl.layers.Dense(n_units=action_dim)(layer)\n",
    "#         input_layer =kl.Input(shape=state_dim)\n",
    "#         layer1 = kl.Dense(12,activation ='elu')(input_layer)\n",
    "#         layer2 = kl.Dense(3)(layer1)\n",
    "\n",
    "        self.model = tl.models.Model(inputs=input_layer, outputs=layer)  # define the model in and out\n",
    "#         self.model = keras.Model(inputs = [input_layer],outputs =[layer2])\n",
    "        self.model.train()\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "    def learn(self, state, action, td_error):  # update weights according to cross_entropy\n",
    "        with tf.GradientTape() as tape:\n",
    "            _logits = self.model(np.array([state]))\n",
    "            _exp_v = tl.rein.cross_entropy_reward_loss(\n",
    "                logits=_logits, actions=[action], rewards=td_error)\n",
    "        grad = tape.gradient(_exp_v, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n",
    "\n",
    "    def get_action(self, state):  # option : greedy para to get the max prob\n",
    "        _logits = self.model(np.array([state]))\n",
    "        _prob = tf.nn.softmax(_logits).numpy()\n",
    "        return tl.rein.choice_action_by_probs(_prob.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic nn\n",
    "class Critic(object):\n",
    "\n",
    "    def __init__(self, state_dim =8, lr=0.01):\n",
    "        input_layer = tl.layers.Input([None, state_dim])\n",
    "        layer = tl.layers.Dense(n_units=16, act=tf.nn.relu)(input_layer) # 12  is the twice of the input dimension\n",
    "        layer = tl.layers.Dense(n_units=1, act=None)(layer)  # output one\n",
    "\n",
    "        self.model = tl.models.Model(inputs=input_layer, outputs=layer)\n",
    "        self.model.train()\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "    def learn(self, state, reward, state_, done):\n",
    "        d = 0 if done else 1\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v = self.model(np.array([state]))\n",
    "            v_ = self.model(np.array([state_]))  # new state  value\n",
    "            td_error = reward + d * gamma * v_ - v\n",
    "            loss = tf.square(td_error)  # MSE\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return td_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000e+03  0.00000e+00  0.00000e+00  2.62000e+02  0.00000e+00\n",
      " -6.66667e+01  4.69697e+01  3.16346e+05]\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing_adding_technical_indicator.add_technical_indicator()\n",
    "env = preprocess_env3_8features.StockEnv(data)\n",
    "LR_A = 0.001  # learning rate for actor\n",
    "LR_C = 0.01  # learning rate for critic\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.95 # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  _inputlayer_1: [None, 8]\n",
      "[TL] Dense  dense_1: 16 relu6\n",
      "[TL] Dense  dense_2: 3 No Activation\n",
      "[TL] Input  _inputlayer_2: [None, 8]\n",
      "[TL] Dense  dense_3: 16 relu\n",
      "[TL] Dense  dense_4: 1 No Activation\n"
     ]
    }
   ],
   "source": [
    "actor = Actor(env.observation_space.shape[0], env.action_space.n, lr=LR_A)\n",
    "critic = Critic(env.observation_space.shape[0], lr=LR_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trade_time_unit(env, obs):  #  using greedy policy\n",
    "#     action = actor.get_action(obs) # get the action\n",
    "#     print(action)\n",
    "#     next_state, reward, done, info = env.step(action)\n",
    "#     return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode():\n",
    "    env.reset()\n",
    "    state = env.reset().astype(np.float32)\n",
    "    reward_total = 0\n",
    "    for step in range(2100):\n",
    "        action = actor.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_total = reward_total + reward\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        td_error = critic.learn(state,reward,next_state,done)\n",
    "        actor.learn(state,action,td_error)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(state)\n",
    "            break\n",
    "    print(\"For this episode,reward total:{}\".format(reward_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000e+00 1.68000e+02 6.43900e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:126.82500000000005\n"
     ]
    }
   ],
   "source": [
    "train_one_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000e+00 1.80000e+02 4.54000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-205.4999999999999\n",
      "[0.00000e+00 1.76000e+02 5.48400e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-40.299999999999955\n",
      "[0.00000e+00 1.70000e+02 5.37100e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-60.075000000000045\n",
      "[0.00000e+00 1.84000e+02 5.32100e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-68.82499999999993\n",
      "[9.10215e+02 0.00000e+00 0.00000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-89.78499999999997\n",
      "[0.00000e+00 1.79000e+02 4.64000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-188.01600000000008\n",
      "[0.00000e+00 1.70000e+02 3.40400e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-404.3\n",
      "[0.00000e+00 1.71000e+02 3.25300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-430.724999999999\n",
      "[0.00000e+00 1.75000e+02 3.74600e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-344.44999999999993\n",
      "[0.00000e+00 1.64000e+02 5.36000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-62.0\n",
      "[0.00000e+00 1.65000e+02 3.63700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-363.525\n",
      "[0.00000e+00 1.68000e+02 2.93100e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-487.07499999999993\n",
      "[0.00000e+00 1.68000e+02 4.09500e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-283.3750000000001\n",
      "[0.00000e+00 1.50000e+02 4.01800e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-296.85000000000014\n",
      "[0.00000e+00 1.75000e+02 3.60300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-369.475\n",
      "[0.00000e+00 1.72000e+02 3.67700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-356.52499999999986\n",
      "[0.00000e+00 1.89000e+02 3.55000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-378.7499999999999\n",
      "[0.00000e+00 1.74000e+02 4.16700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-270.775\n",
      "[0.00000e+00 1.76000e+02 4.11300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-280.2249999999999\n",
      "[0.00000e+00 1.78000e+02 3.61800e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-366.85\n",
      "[0.00000e+00 1.73000e+02 3.94800e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-309.10000000000014\n",
      "[0.00000e+00 2.39000e+02 3.96100e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-306.82500000000005\n",
      "[0.00000e+00 1.75000e+02 3.88200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-320.65\n",
      "[0.00000e+00 2.11000e+02 3.81700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.025\n",
      "[0.00000e+00 2.62000e+02 3.81700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.025\n",
      "[0.00000e+00 1.59000e+02 3.53200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-381.9\n",
      "[0.00000e+00 1.94000e+02 3.59800e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-370.35\n",
      "[0.00000e+00 1.76000e+02 3.88200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-320.65\n",
      "[0.00000e+00 1.88000e+02 4.15600e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-272.70000000000005\n",
      "[0.00000e+00 1.70000e+02 3.85300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-325.725\n",
      "[0.00000e+00 1.78000e+02 3.71300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-350.225\n",
      "[0.00000e+00 1.94000e+02 3.56100e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-376.82499999999993\n",
      "[0.00000e+00 2.03000e+02 3.92500e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-313.1249999999999\n",
      "[0.00000e+00 2.48000e+02 3.83200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-329.4\n",
      "[0.00000e+00 1.83000e+02 3.28000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-426.0000000000001\n",
      "[0.00000e+00 1.80000e+02 4.05700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-290.025\n",
      "[0.00000e+00 1.92000e+02 3.81200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.9000000000001\n",
      "[0.00000e+00 2.62000e+02 3.81700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.025\n",
      "[0.00000e+00 1.99000e+02 3.99000e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-301.75\n",
      "[0.00000e+00 2.62000e+02 3.81700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.025\n",
      "[0.00000e+00 1.81000e+02 3.94200e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-310.15\n",
      "[0.00000e+00 2.62000e+02 3.81700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-332.025\n",
      "[0.00000e+00 1.85000e+02 3.53600e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-381.20000000000005\n",
      "[0.00000e+00 1.97000e+02 3.89300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-318.72500000000014\n",
      "[0.00000e+00 1.80000e+02 3.75400e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-343.05000000000007\n",
      "[0.00000e+00 2.02000e+02 3.99300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-301.225\n",
      "[0.00000e+00 2.08000e+02 3.87700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-321.5250000000001\n",
      "[0.00000e+00 1.78000e+02 3.49700e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-388.0250000000001\n",
      "[0.00000e+00 1.74000e+02 3.56300e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-376.4749999999999\n",
      "[0.00000e+00 1.77000e+02 3.83400e+00 1.77000e+02 5.20000e-02 8.72270e+00\n",
      " 5.68858e+01 3.12915e+05]\n",
      "For this episode,reward total:-329.04999999999995\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    train_one_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7b593aacd241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_one_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-264da1f41a65>\u001b[0m in \u001b[0;36mtrain_one_episode\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mreward_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_total\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-34beac34283e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, state, reward, state_, done)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mv_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# MSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstFirstOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1619\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1620\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstSecondOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1621\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[1;31m# No gradient skipping, so do the full gradient computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGradAgainstSecondOnly\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1599\u001b[0m   \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5605\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5606\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5607\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5608\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_one_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-358.7786"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(env.reward_memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
